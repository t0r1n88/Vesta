{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1ddccf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.\tНужно чтобы программа обновляла данные в столбцах которые есть в первой таблице при совпадении идентификатора.\n",
    "2.\tДобавляла данные в соответствующие столбцы если такого идентификатора нет в первой таблице\n",
    "Скорее всего нужно создавать дополнительный лист в главном файле и сохранять его отдельно, так будет проще\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import openpyxl\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from openpyxl.styles import Font\n",
    "from openpyxl.styles import Alignment\n",
    "from openpyxl import load_workbook\n",
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "from tkinter import messagebox\n",
    "from tkinter import ttk\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "from dateutil.parser import ParserError\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='openpyxl')\n",
    "pd.options.mode.chained_assignment = None\n",
    "import sys\n",
    "import locale\n",
    "import logging\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1949d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_params_columns_to_int(lst):\n",
    "    \"\"\"\n",
    "    Функция для конвератации значений колонок которые нужно обработать.\n",
    "    Очищает от пустых строк, чтобы в итоге остался список из чисел в формате int\n",
    "    \"\"\"\n",
    "    out_lst = [] # Создаем список в который будем добавлять только числа\n",
    "    for value in lst: # Перебираем список\n",
    "        try:\n",
    "            # Обрабатываем случай с нулем, для того чтобы после приведения к питоновскому отсчету от нуля не получилась колонка с номером -1\n",
    "            number = int(value)\n",
    "            if number != 0:\n",
    "                out_lst.append(value) # Если конвертирования прошло без ошибок то добавляем\n",
    "            else:\n",
    "                continue\n",
    "        except: # Иначе пропускаем\n",
    "            continue\n",
    "    return out_lst\n",
    "\n",
    "\n",
    "def convert_columns_to_str(df, number_columns):\n",
    "    \"\"\"\n",
    "    Функция для конвертации указанных столбцов в строковый тип и очистки от пробельных символов в начале и конце\n",
    "    \"\"\"\n",
    "\n",
    "    for column in number_columns:  # Перебираем список нужных колонок\n",
    "        try:\n",
    "            df.iloc[:, column] = df.iloc[:, column].astype(str)\n",
    "            # Очищаем колонку от пробельных символов с начала и конца\n",
    "            df.iloc[:, column] = df.iloc[:, column].apply(lambda x: x.strip())\n",
    "        except IndexError:\n",
    "            messagebox.showerror('Веста Обработка таблиц и создание документов ver 1.21',\n",
    "                                 'Проверьте порядковые номера колонок которые вы хотите обработать.')\n",
    "            \n",
    "def processing_date_column(df, lst_columns):\n",
    "    \"\"\"\n",
    "    Функция для обработки столбцов с датами. конвертация в строку формата ДД.ММ.ГГГГ\n",
    "    \"\"\"\n",
    "    # получаем первую строку\n",
    "    first_row = df.iloc[0, lst_columns]\n",
    "\n",
    "    lst_first_row = list(first_row)  # Превращаем строку в список\n",
    "    lst_date_columns = []  # Создаем список куда будем сохранять колонки в которых находятся даты\n",
    "    tupl_row = list(zip(lst_columns,\n",
    "                        lst_first_row))  # Создаем список кортежей формата (номер колонки,значение строки в этой колонке)\n",
    "\n",
    "    for idx, value in tupl_row:  # Перебираем кортеж\n",
    "        result = check_date_columns(idx, value)  # проверяем является ли значение датой\n",
    "        if result:  # если да то добавляем список порядковый номер колонки\n",
    "            lst_date_columns.append(result)\n",
    "        else:  # иначе проверяем следующее значение\n",
    "            continue\n",
    "    for i in lst_date_columns:  # Перебираем список с колонками дат, превращаем их в даты и конвертируем в нужный строковый формат\n",
    "        df.iloc[:, i] = pd.to_datetime(df.iloc[:, i], errors='coerce', dayfirst=True)\n",
    "        df.iloc[:, i] = df.iloc[:, i].apply(create_doc_convert_date)\n",
    "\n",
    "def check_date_columns(i, value):\n",
    "    \"\"\"\n",
    "    Функция для проверки типа колонки. Необходимо найти колонки с датой\n",
    "    :param i:\n",
    "    :param value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        itog = pd.to_datetime(str(value), infer_datetime_format=True)\n",
    "\n",
    "    except ParserError:\n",
    "        pass\n",
    "    except ValueError:\n",
    "        pass\n",
    "    except TypeError:\n",
    "        pass\n",
    "    else:\n",
    "        return i\n",
    "\n",
    "def create_doc_convert_date(cell):\n",
    "    \"\"\"\n",
    "    Функция для конвертации даты при создании документов\n",
    "    :param cell:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        string_date = datetime.datetime.strftime(cell, '%d.%m.%Y')\n",
    "        return string_date\n",
    "    except ValueError:\n",
    "        return 'Не удалось конвертировать дату.Проверьте значение ячейки!!!'\n",
    "    except TypeError:\n",
    "        return 'Не удалось конвертировать дату.Проверьте значение ячейки!!!'    \n",
    "    \n",
    "def clean_ending_columns(lst_columns:list,name_first_df,name_second_df):\n",
    "    \"\"\"\n",
    "    Функция для очистки колонок таблицы с совпадающими данными от окончаний _x _y\n",
    "\n",
    "    :param lst_columns:\n",
    "    :param time_generate\n",
    "    :param name_first_df\n",
    "    :param name_second_df\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    out_columns = [] # список для очищенных названий\n",
    "    for name_column in lst_columns:\n",
    "        if '_x' in name_column:\n",
    "            # если они есть то проводим очистку и добавление времени\n",
    "            cut_name_column = name_column[:-2] # обрезаем\n",
    "            temp_name = f'{cut_name_column}_{name_first_df}' # соединяем\n",
    "            out_columns.append(temp_name) # добавляем\n",
    "        elif '_y' in name_column:\n",
    "            cut_name_column = name_column[:-2]  # обрезаем\n",
    "            temp_name = f'{cut_name_column}_{name_second_df}'  # соединяем\n",
    "            out_columns.append(temp_name)  # добавляем\n",
    "        else:\n",
    "            out_columns.append(name_column)\n",
    "    return out_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380fb6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0339c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получаем значения текстовых полей\n",
    "# first_sheet_name = str(entry_first_sheet_name.get())\n",
    "# second_sheet_name = str(entry_second_sheet_name.get())\n",
    "\n",
    "# # загружаем файлы\n",
    "# first_df = pd.read_excel(name_first_file_comparison, sheet_name=first_sheet_name, dtype=str,\n",
    "#                          keep_default_na=False)\n",
    "# # получаем имя файла\n",
    "# name_first_df = name_first_file_comparison.split('/')[-1]\n",
    "# name_first_df = name_first_df.split('.xlsx')[0]\n",
    "\n",
    "# second_df = pd.read_excel(name_second_file_comparison, sheet_name=second_sheet_name, dtype=str,\n",
    "#                           keep_default_na=False)\n",
    "# # получаем имя файла\n",
    "# name_second_df = name_second_file_comparison.split('/')[-1]\n",
    "# name_second_df = name_second_df.split('.xlsx')[0]\n",
    "\n",
    "# params = pd.read_excel(file_params, header=None, keep_default_na=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5adab280",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sheet_name = 'Worksheet'\n",
    "second_sheet_name = 'Лист1'\n",
    "# first_sheet_name = 'Sheet1'\n",
    "# second_sheet_name = 'Sheet1'\n",
    "name_first_file_comparison = 'data/Основная таблица.xlsx'\n",
    "name_second_file_comparison = 'data/Социалка.xlsx'\n",
    "\n",
    "# name_first_file_comparison = 'data/Таблица с обновленными данными и колонками от 13_46_50.xlsx'\n",
    "# name_second_file_comparison = 'data/Социалка 2.xlsx'\n",
    "\n",
    "file_params = 'data/Параметры слияния.xlsx'\n",
    "path_to_end_folder_comparison = 'data'\n",
    "\n",
    "first_df = pd.read_excel(name_first_file_comparison, sheet_name=first_sheet_name, dtype=str,\n",
    "                         keep_default_na=False)\n",
    "# получаем имя файла\n",
    "name_first_df = name_first_file_comparison.split('/')[-1]\n",
    "name_first_df = name_first_df.split('.xlsx')[0]\n",
    "\n",
    "second_df = pd.read_excel(name_second_file_comparison, sheet_name=second_sheet_name, dtype=str,\n",
    "                          keep_default_na=False)\n",
    "# получаем имя файла\n",
    "name_second_df = name_second_file_comparison.split('/')[-1]\n",
    "name_second_df = name_second_df.split('.xlsx')[0]\n",
    "\n",
    "params = pd.read_excel(file_params, header=None, keep_default_na=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "86668ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Преврашаем каждую колонку в список\n",
    "params_first_columns = params[0].tolist()\n",
    "params_second_columns = params[1].tolist()\n",
    "\n",
    "# Конвертируем в инт заодно проверяя корректность введенных данных\n",
    "int_params_first_columns = convert_params_columns_to_int(params_first_columns)\n",
    "int_params_second_columns = convert_params_columns_to_int(params_second_columns)\n",
    "\n",
    "# Отнимаем 1 от каждого значения чтобы привести к питоновским индексам\n",
    "int_params_first_columns = list(map(lambda x: x - 1, int_params_first_columns))\n",
    "int_params_second_columns = list(map(lambda x: x - 1, int_params_second_columns))\n",
    "\n",
    "# Конвертируем нужные нам колонки в str\n",
    "convert_columns_to_str(first_df, int_params_first_columns)\n",
    "convert_columns_to_str(second_df, int_params_second_columns)\n",
    "\n",
    "# в этом месте конвертируем даты в формат ДД.ММ.ГГГГ\n",
    "processing_date_column(first_df, int_params_first_columns)\n",
    "processing_date_column(second_df, int_params_second_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "179369f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверяем наличие колонки _merge\n",
    "if '_merge' in first_df.columns:\n",
    "    first_df.drop(columns=['_merge'], inplace=True)\n",
    "if '_merge' in second_df.columns:\n",
    "    second_df.drop(columns=['_merge'], inplace=True)\n",
    "# Проверяем наличие колонки ID\n",
    "if 'ID_объединения' in first_df.columns:\n",
    "    first_df.drop(columns=['ID_объединения'], inplace=True)\n",
    "if 'ID_объединения' in second_df.columns:\n",
    "    second_df.drop(columns=['ID_объединения'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0fffd2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Создаем в каждом датафрейме колонку с айди путем склеивания всех нужных колонок в одну строку\n",
    "first_df['ID_объединения'] = first_df.iloc[:, int_params_first_columns].sum(axis=1)\n",
    "second_df['ID_объединения'] = second_df.iloc[:, int_params_second_columns].sum(axis=1)\n",
    "# Удаляем все пробелы чтобы они не повлияли на слияние\n",
    "first_df['ID_объединения'] = first_df['ID_объединения'].apply(lambda x: x.replace(' ', ''))\n",
    "second_df['ID_объединения'] = second_df['ID_объединения'].apply(lambda x: x.replace(' ', ''))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a9c4e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Обрабатываем дубликаты\n",
    "\n",
    "duplicates_first_df = first_df[first_df.duplicated(subset=['ID_объединения'],\n",
    "                                                   keep=False)]  # Сохраняем все значения у которых есть дубликаты в отдельный датафрейм\n",
    "\n",
    "first_df.drop_duplicates(subset=['ID_объединения'], keep=False, inplace=True)  # Удаляем дубликаты из датафрейма\n",
    "\n",
    "duplicates_second_df = second_df[second_df.duplicated(subset=['ID_объединения'],\n",
    "                                                      keep=False)]  # Сохраняем все значения у которых есть дубликаты в отдельный датафрейм\n",
    "second_df.drop_duplicates(subset=['ID_объединения'], keep=False, inplace=True)  # Удаляем дубликаты из датафрейма\n",
    "\n",
    "# # Проверяем размер датафрейма с дубликатами, если он больше 0 то выдаем сообшение пользователю\n",
    "if duplicates_first_df.shape[0] > 0:\n",
    "    messagebox.showwarning('Веста Обработка таблиц и создание документов ver 1.21',\n",
    "                           f'В первой таблице обнаружены дубликаты!!!\\nДля корректного объединения таблиц ,дубликаты перенесены в отдельный лист итоговой таблицы')\n",
    "if duplicates_second_df.shape[0] > 0:\n",
    "    messagebox.showwarning('Веста Обработка таблиц и создание документов ver 1.21',\n",
    "                           f'Во второй таблице обнаружены дубликаты!!!\\nДля корректного объединения таблиц ,дубликаты перенесены в отдельный лист итоговой таблицы')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "58ec2a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Worksheet \"Дубликаты вторая таблица\">"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# В результат объединения попадают совпадающие по ключу записи обеих таблиц и все строки из этих двух таблиц, для которых пар не нашлось. Порядок таблиц в запросе не\n",
    "\n",
    "# Создаем документ\n",
    "wb = openpyxl.Workbook()\n",
    "# создаем листы\n",
    "ren_sheet = wb['Sheet']\n",
    "ren_sheet.title = 'Таблица 1'\n",
    "wb.create_sheet(title='Таблица 2', index=1)\n",
    "wb.create_sheet(title='Совпадающие данные', index=2)\n",
    "wb.create_sheet(title='Обновленная таблица', index=3)\n",
    "wb.create_sheet(title='Объединённая таблица', index=4)\n",
    "# Создаем листы для дубликатов\n",
    "wb.create_sheet(title='Дубликаты первая таблица', index=5)\n",
    "wb.create_sheet(title='Дубликаты вторая таблица', index=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "95d5e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем переменные содержащие в себе количество колонок в базовых датареймах\n",
    "first_df_quantity_cols = len(first_df.columns)  # не забываем что там добавилась колонка ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0fd27d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Проводим слияние\n",
    "itog_df = pd.merge(first_df, second_df, how='outer', left_on=['ID_объединения'], right_on=['ID_объединения'],\n",
    "                   indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2af14507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# копируем в отдельный датафрейм для создания таблицы с обновлениями\n",
    "update_df = itog_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2433c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Записываем каждый датафрейм в соответсвующий лист\n",
    "# Левая таблица\n",
    "left_df = itog_df[itog_df['_merge'] == 'left_only']\n",
    "left_df.drop(['_merge'], axis=1, inplace=True)\n",
    "\n",
    "# Удаляем колонки второй таблицы чтобы не мешались\n",
    "left_df.drop(left_df.iloc[:, first_df_quantity_cols:], axis=1, inplace=True)\n",
    "\n",
    "# Переименовываем колонки у которых были совпадение во второй таблице, в таких колонках есть добавление _x\n",
    "clean_left_columns = list(map(lambda x: x[:-2] if '_x' in x else x, list(left_df.columns)))\n",
    "left_df.columns = clean_left_columns\n",
    "for r in dataframe_to_rows(left_df, index=False, header=True):\n",
    "    wb['Таблица 1'].append(r)\n",
    "\n",
    "right_df = itog_df[itog_df['_merge'] == 'right_only']\n",
    "right_df.drop(['_merge'], axis=1, inplace=True)\n",
    "\n",
    "# Удаляем колонки первой таблицы таблицы чтобы не мешались\n",
    "right_df.drop(right_df.iloc[:, :first_df_quantity_cols - 1], axis=1, inplace=True)\n",
    "\n",
    "# Переименовываем колонки у которых были совпадение во второй таблице, в таких колонках есть добавление _x\n",
    "clean_right_columns = list(map(lambda x: x[:-2] if '_y' in x else x, list(right_df.columns)))\n",
    "right_df.columns = clean_right_columns\n",
    "\n",
    "for r in dataframe_to_rows(right_df, index=False, header=True):\n",
    "    wb['Таблица 2'].append(r)\n",
    "\n",
    "both_df = itog_df[itog_df['_merge'] == 'both']\n",
    "both_df.drop(['_merge'], axis=1, inplace=True)\n",
    "# Очищаем от _x  и _y\n",
    "clean_both_columns = clean_ending_columns(list(both_df.columns), name_first_df, name_second_df)\n",
    "both_df.columns = clean_both_columns\n",
    "\n",
    "for r in dataframe_to_rows(both_df, index=False, header=True):\n",
    "    wb['Совпадающие данные'].append(r)\n",
    "\n",
    "# Сохраняем общую таблицу\n",
    "# Заменяем названия индикаторов на более понятные\n",
    "itog_df['_merge'] = itog_df['_merge'].apply(lambda x: 'Данные из первой таблицы' if x == 'left_only' else\n",
    "('Данные из второй таблицы' if x == 'right_only' else 'Совпадающие данные'))\n",
    "itog_df['_merge'] = itog_df['_merge'].astype(str)\n",
    "\n",
    "clean_itog_df = clean_ending_columns(list(itog_df.columns), name_first_df, name_second_df)\n",
    "itog_df.columns = clean_itog_df\n",
    "for r in dataframe_to_rows(itog_df, index=False, header=True):\n",
    "    wb['Объединённая таблица'].append(r)\n",
    "\n",
    "# Записываем дубликаты в соответствующие листы\n",
    "for r in dataframe_to_rows(duplicates_first_df, index=False, header=True):\n",
    "    wb['Дубликаты первая таблица'].append(r)\n",
    "\n",
    "for r in dataframe_to_rows(duplicates_second_df, index=False, header=True):\n",
    "    wb['Дубликаты вторая таблица'].append(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "963bf52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# получаем список с совпадающими колонками первой таблицы\n",
    "first_df_columns = [column for column in list(update_df.columns) if str(column).endswith('_x')]\n",
    "# получаем список с совпадающими колонками второй таблицы\n",
    "second_df_columns = [column for column in list(update_df.columns) if str(column).endswith('_y')]\n",
    "# Создаем из списка совпадающих колонок второй таблицы словарь, чтобы было легче обрабатывать\n",
    "# да конечно можно было сделать в одном выражении но как я буду читать это через 2 недели?\n",
    "dct_second_columns = {column.split('_y')[0]:column for column in second_df_columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "50e20607",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in first_df_columns:\n",
    "    # очищаем от _x\n",
    "    name_column = column.split('_x')[0]\n",
    "    # Обновляем значение в случае если в колонке _merge стоит both, иначе оставляем старое значение,\n",
    "    # Чтобы обновить значение в ячейке, во второй таблице не должно быть пустого значения или пробела в аналогичной колонке\n",
    "    \n",
    "    update_df[column] = np.where((update_df['_merge']=='both') & (update_df[dct_second_columns[name_column]]) & (update_df[dct_second_columns[name_column]] != ' '),update_df[dct_second_columns[name_column]],update_df[column]) \n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d9e98c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b110c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "050e8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаляем колонки с _y\n",
    "update_df.drop(columns=[column for column in update_df.columns if column.endswith('_y')],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "49b1926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переименовываем колонки с _x\n",
    "update_df.columns = list(map(lambda x:x[:-2] if x.endswith('_x') else x,update_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1ba71c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаляем строки с _merge == right_only\n",
    "update_df = update_df[update_df['_merge'] != 'right_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "03aab2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаляем служебные колонки\n",
    "update_df.drop(columns=['ID_объединения','_merge'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "10c7daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаляем лишнюю колонку в right_df\n",
    "right_df.drop(columns=['ID_объединения'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "25e1b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем нехватающие колонки\n",
    "new_right_df = right_df.reindex(columns=update_df.columns,fill_value=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "594d2efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_df = pd.concat([update_df,new_right_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e0bc285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in dataframe_to_rows(update_df, index=False, header=True):\n",
    "    wb['Обновленная таблица'].append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59addba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e468f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bb06312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# генерируем текущее время\n",
    "t = time.localtime()\n",
    "current_time = time.strftime('%H_%M_%S', t)\n",
    "# Сохраняем итоговый файл\n",
    "wb.save(f'{path_to_end_folder_comparison}/Результат слияния 2 таблиц от {current_time}.xlsx')\n",
    "# Сохраняем отдельно обновленную таблицу\n",
    "update_df.to_excel(f'{path_to_end_folder_comparison}/Таблица с обновленными данными и колонками от {current_time}.xlsx',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba8b8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d250a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197564ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2972d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40210f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5067cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340af2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba1865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeccb615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac6f712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02624d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d2792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d35ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f3b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d4acdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b583ec85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d65c90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cac6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fdb008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
